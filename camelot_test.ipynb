{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7e884df-57d2-4daa-b7de-b5b676ff19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSimilarTables(data):\n",
    "    dfs = []\n",
    "    dfs_by_column_count = {}\n",
    "    \n",
    "    for i, table in enumerate(data):\n",
    "        df = table.df\n",
    "        num_columns = len(df.columns)\n",
    "    \n",
    "        # Set the first row as the header (column names) and drop it from the data\n",
    "        df.columns = df.iloc[0]  # Set the first row as column names\n",
    "        df = df[1:]  # Drop the first row from the data\n",
    "    \n",
    "        # Get number of columns\n",
    "        num_columns = len(df.columns)\n",
    "        \n",
    "        if num_columns in dfs_by_column_count:\n",
    "            # Combine the existing DataFrame with the new one\n",
    "            dfs_by_column_count[num_columns] = pd.concat([dfs_by_column_count[num_columns], df], ignore_index=True)\n",
    "        else:\n",
    "            # Initialize the entry with the current DataFrame\n",
    "            dfs_by_column_count[num_columns] = df\n",
    "    return dfs_by_column_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486453fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables: Ghostscript is not installed. You can install it using the instructions here: https://camelot-py.readthedocs.io/en/master/user/install-deps.html\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, pages='all'):\n",
    "    \"\"\"\n",
    "    Extract tables from PDF file using Camelot.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF file\n",
    "        pages (str): Page numbers to process (e.g., '1,2,3' or 'all')\n",
    "    \n",
    "    Returns:\n",
    "        list: List of pandas DataFrames, one for each table found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract tables using both lattice and stream modes\n",
    "        tables_lattice = camelot.read_pdf(pdf_path, pages=pages, flavor='lattice')\n",
    "        tables_stream = camelot.read_pdf(pdf_path, pages=pages, flavor='stream')\n",
    "        \n",
    "        print(f\"Found {len(tables_lattice)} tables using lattice mode\")\n",
    "        print(f\"Found {len(tables_stream)} tables using stream mode\")\n",
    "        \n",
    "        # # Convert to list of dataframes\n",
    "        # dfs_lattice = [table.df for table in tables_lattice]\n",
    "        # dfs_stream = [table.df for table in tables_stream]\n",
    "        \n",
    "        # # Combine results from both modes\n",
    "        # # all_dfs = dfs_lattice + dfs_stream\n",
    "        \n",
    "        return [tables_lattice, tables_stream]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with multiple tables in a PDF\n",
    "    pdf_path = \"classification_model/loss_runs/input/Loss_Run___len stoler 8-24.pdf\"\n",
    "\n",
    "    # Extract tables\n",
    "    raw_tables = extract_tables_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7702d8bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p5/f6blwg_j4855svrp3wyc2ddc0000gn/T/ipykernel_28556/1630402241.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtables_lattice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstream_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombineSimilarTables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtables_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/p5/f6blwg_j4855svrp3wyc2ddc0000gn/T/ipykernel_28556/663465542.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdfs_by_column_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnum_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Set the first row as the header (column names) and drop it from the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UW-Madison_Undergraduate/Understory2/understory-venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "tables_lattice, tables_stream = raw_tables\n",
    "stream_dfs = combineSimilarTables(tables_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c467bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcamelot\u001b[39;00m  \u001b[38;5;66;03m# For table extraction from PDFs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m  \u001b[38;5;66;03m# For general PDF text extraction\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer  \u001b[38;5;66;03m# For semantic similarity\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import camelot  # For table extraction from PDFs\n",
    "import pdfplumber  # For general PDF text extraction\n",
    "from sentence_transformers import SentenceTransformer  # For semantic similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import spacy\n",
    "\n",
    "class PDFTableExtractor:\n",
    "    def __init__(self):\n",
    "        # Initialize models and target columns\n",
    "        self.target_columns = [\n",
    "            'location_id', 'carrier', 'claim_number', 'status', 'loss_cause',\n",
    "            'occurred_at', 'expense', 'incurred', 'recovery', 'loss_amount', 'description'\n",
    "        ]\n",
    "        \n",
    "        # Load models\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")  # For general NLP tasks\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # For semantic similarity\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Calculate embeddings for target columns once\n",
    "        self.target_embeddings = self.sentence_model.encode(self.target_columns)\n",
    "\n",
    "    def extract_tables_from_pdf(self, pdf_path: str) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Extract tables from PDF using multiple methods and combine results.\n",
    "        \"\"\"\n",
    "        tables = []\n",
    "        \n",
    "        # Try Camelot first (good for structured tables)\n",
    "        try:\n",
    "            camelot_tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')\n",
    "            tables.extend([table.df for table in camelot_tables])\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Camelot extraction failed: {str(e)}\")\n",
    "\n",
    "        # Fallback to pdfplumber (better for less structured tables)\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    extracted_tables = page.extract_tables()\n",
    "                    for table in extracted_tables:\n",
    "                        df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                        tables.append(df)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"PDFPlumber extraction failed: {str(e)}\")\n",
    "\n",
    "        return tables\n",
    "\n",
    "    def preprocess_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean and standardize column names.\n",
    "        \"\"\"\n",
    "        def clean_column(col):\n",
    "            # Convert to lowercase and remove special characters\n",
    "            col = str(col).lower().strip()\n",
    "            col = re.sub(r'[^a-z0-9\\s]', '', col)\n",
    "            col = re.sub(r'\\s+', '_', col)\n",
    "            return col\n",
    "\n",
    "        df.columns = [clean_column(col) for col in df.columns]\n",
    "        return df\n",
    "\n",
    "    def find_best_column_matches(self, df: pd.DataFrame) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Use semantic similarity to match source columns to target columns.\n",
    "        \"\"\"\n",
    "        column_mappings = {}\n",
    "        source_embeddings = self.sentence_model.encode(df.columns.tolist())\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(source_embeddings, self.target_embeddings)\n",
    "        \n",
    "        # Find best matches\n",
    "        for target_idx, target_col in enumerate(self.target_columns):\n",
    "            best_match_idx = np.argmax(similarity_matrix[:, target_idx])\n",
    "            source_col = df.columns[best_match_idx]\n",
    "            score = similarity_matrix[best_match_idx, target_idx]\n",
    "            \n",
    "            # Only map if similarity is above threshold\n",
    "            if score > 0.5:\n",
    "                column_mappings[target_col] = source_col\n",
    "                \n",
    "        return column_mappings\n",
    "\n",
    "    def extract_value(self, text: str, value_type: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract and normalize specific types of values.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return None\n",
    "            \n",
    "        text = str(text).strip()\n",
    "        \n",
    "        if value_type == 'amount':\n",
    "            # Extract monetary amounts\n",
    "            amount_match = re.search(r'[\\$]?([0-9,]+\\.?[0-9]*)', text)\n",
    "            if amount_match:\n",
    "                return float(amount_match.group(1).replace(',', ''))\n",
    "                \n",
    "        elif value_type == 'date':\n",
    "            # Try multiple date formats\n",
    "            date_patterns = [\n",
    "                '%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', \n",
    "                '%b %d %Y', '%B %d %Y', '%Y/%m/%d'\n",
    "            ]\n",
    "            for pattern in date_patterns:\n",
    "                try:\n",
    "                    return datetime.strptime(text, pattern).strftime('%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                    \n",
    "        return text\n",
    "\n",
    "    def process_table(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process and normalize extracted table data.\n",
    "        \"\"\"\n",
    "        # Preprocess column names\n",
    "        df = self.preprocess_column_names(df)\n",
    "        \n",
    "        # Find best column matches\n",
    "        column_mappings = self.find_best_column_matches(df)\n",
    "        \n",
    "        # Create new DataFrame with target columns\n",
    "        result_df = pd.DataFrame(columns=self.target_columns)\n",
    "        \n",
    "        # Map and process values\n",
    "        for target_col in self.target_columns:\n",
    "            if target_col in column_mappings:\n",
    "                source_col = column_mappings[target_col]\n",
    "                \n",
    "                # Apply appropriate extraction based on column type\n",
    "                if target_col in ['expense', 'incurred', 'recovery', 'loss_amount']:\n",
    "                    result_df[target_col] = df[source_col].apply(\n",
    "                        lambda x: self.extract_value(x, 'amount')\n",
    "                    )\n",
    "                elif target_col == 'occurred_at':\n",
    "                    result_df[target_col] = df[source_col].apply(\n",
    "                        lambda x: self.extract_value(x, 'date')\n",
    "                    )\n",
    "                else:\n",
    "                    result_df[target_col] = df[source_col].apply(str)\n",
    "            \n",
    "        return result_df\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main method to process a PDF and return structured data.\n",
    "        \"\"\"\n",
    "        # Extract tables\n",
    "        tables = self.extract_tables_from_pdf(pdf_path)\n",
    "        \n",
    "        if not tables:\n",
    "            self.logger.warning(f\"No tables found in {pdf_path}\")\n",
    "            return pd.DataFrame(columns=self.target_columns)\n",
    "        \n",
    "        # Process each table and combine results\n",
    "        processed_tables = []\n",
    "        for table in tables:\n",
    "            processed_df = self.process_table(table)\n",
    "            if not processed_df.empty:\n",
    "                processed_tables.append(processed_df)\n",
    "        \n",
    "        # Combine all processed tables\n",
    "        if processed_tables:\n",
    "            final_df = pd.concat(processed_tables, ignore_index=True)\n",
    "            return final_df\n",
    "        \n",
    "        return pd.DataFrame(columns=self.target_columns)\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process multiple PDFs and combine results into a single CSV file.\n",
    "    \"\"\"\n",
    "    extractor = PDFTableExtractor()\n",
    "    pdf_files = Path(pdf_directory).glob('*.pdf')\n",
    "    all_results = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            result_df = extractor.process_pdf(str(pdf_file))\n",
    "            if not result_df.empty:\n",
    "                # Add source file information\n",
    "                result_df['source_file'] = pdf_file.name\n",
    "                all_results.append(result_df)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pdf_file}: {str(e)}\")\n",
    "    \n",
    "    if all_results:\n",
    "        # Combine all results\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        # Save to CSV\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Results saved to {output_path}\")\n",
    "    else:\n",
    "        logging.warning(\"No data extracted from PDFs\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Process a directory of PDFs\n",
    "    process_multiple_pdfs(\n",
    "        pdf_directory=\"./samples/input/\",\n",
    "        output_path=\"extracted_data.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a4d20-6df8-47d3-84e6-83d6e86791c1",
   "metadata": {},
   "source": [
    "# PDF Plumber Test\n",
    "- First, use extract tables to get the tables off PDF\n",
    "- Then, convert to dataframes and attempt to clean\n",
    "- Compare with that produced by camelot\n",
    "- Concept: Fallback to LLM only if local installations fail to deliver expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b2b3ff1-4f57-4557-8a7a-66b4c35054e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Path to the uploaded PDF\n",
    "pdf_path = \"./samples/input/Loss_Run___len stoler 8-24.pdf\"\n",
    "\n",
    "# Extract tables from the PDF\n",
    "extracted_data = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        # Extract tables from the current page\n",
    "        tables = page.extract_tables()\n",
    "        for table in tables:\n",
    "            extracted_data.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dde12223-53d4-4a98-b388-2cadfda2f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "dfs_by_column_count = {}\n",
    "\n",
    "for i, table in enumerate(extracted_data):\n",
    "    df = pd.DataFrame(table)\n",
    "    num_columns = len(df.columns)\n",
    "\n",
    "    # Set the first row as the header (column names) and drop it from the data\n",
    "    df.columns = df.iloc[0]  # Set the first row as column names\n",
    "    df = df[1:]  # Drop the first row from the data\n",
    "\n",
    "    # Get number of columns\n",
    "    num_columns = len(df.columns)\n",
    "    \n",
    "    if num_columns in dfs_by_column_count:\n",
    "        # Combine the existing DataFrame with the new one\n",
    "        dfs_by_column_count[num_columns] = pd.concat([dfs_by_column_count[num_columns], df], ignore_index=True)\n",
    "    else:\n",
    "        # Initialize the entry with the current DataFrame\n",
    "        dfs_by_column_count[num_columns] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "531cd9d3-d991-48fc-b0af-d16f1c2f9313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Sub Coverage</th>\n",
       "      <th>Val ID\\nDescription</th>\n",
       "      <th>Claim\\nNumber</th>\n",
       "      <th>Date of\\nLoss</th>\n",
       "      <th>Status</th>\n",
       "      <th>Claimant\\nName</th>\n",
       "      <th>Accident Narrative</th>\n",
       "      <th>Paid\\nIndemnity</th>\n",
       "      <th>Paid\\nExpense</th>\n",
       "      <th>Reserves\\nTotal</th>\n",
       "      <th>Claim\\nRecovery</th>\n",
       "      <th>Net\\nIncurred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nLIABILITY</td>\n",
       "      <td>NO FAULT BI\\nMEDICAL</td>\n",
       "      <td>4340183468</td>\n",
       "      <td>20211116</td>\n",
       "      <td>Opened</td>\n",
       "      <td>Jackson-Smith\\nIris</td>\n",
       "      <td>\"PIP LOSS\" IV driver\\nexited the parked IV\\nwh...</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$2,926.89</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>Total\\n$2,926.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nLIABILITY</td>\n",
       "      <td>NO FAULT BI\\nMEDICAL</td>\n",
       "      <td>4340183468</td>\n",
       "      <td>20211116</td>\n",
       "      <td>Opened</td>\n",
       "      <td>Smith Edward</td>\n",
       "      <td>\"PIP LOSS\" IV driver\\nexited the parked IV\\nwh...</td>\n",
       "      <td>$46,264.65</td>\n",
       "      <td>$4,063.15</td>\n",
       "      <td>$7,990.94</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$58,318.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nPHYSICAL\\nDAMAGE</td>\n",
       "      <td>COLLISION</td>\n",
       "      <td>1340152292</td>\n",
       "      <td>20211116</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Westbury\\nToyota</td>\n",
       "      <td>IV driver exited the\\nparked IV when an OV\\nth...</td>\n",
       "      <td>$34,815.60</td>\n",
       "      <td>$165.30</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>##########</td>\n",
       "      <td>$6,879.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nPHYSICAL\\nDAMAGE</td>\n",
       "      <td>COLLISION</td>\n",
       "      <td>1510183901</td>\n",
       "      <td>20220326</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Westbury\\nToyota</td>\n",
       "      <td>V2 r/e V1</td>\n",
       "      <td>$6,489.19</td>\n",
       "      <td>$131.85</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$6,621.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nPHYSICAL\\nDAMAGE</td>\n",
       "      <td>NEW VEHICLE\\nCOMPREHENSIVE</td>\n",
       "      <td>1510178306</td>\n",
       "      <td>20211212</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Westbury\\nToyota</td>\n",
       "      <td>suspect came on lot\\nand lit two vehicles on\\n...</td>\n",
       "      <td>$44,050.82</td>\n",
       "      <td>$132.50</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$44,183.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>AUTOMOBILE</td>\n",
       "      <td>AUTO\\nPHYSICAL\\nDAMAGE</td>\n",
       "      <td>SERVICE VEHICLE\\nCOMP</td>\n",
       "      <td>1510207883</td>\n",
       "      <td>20231127</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Len Stoler\\nAutomotiv e</td>\n",
       "      <td>water leak in loaner\\nvehicle</td>\n",
       "      <td>$2,321.21</td>\n",
       "      <td>$125.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$2,446.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>GENERAL\\nLIABILITY</td>\n",
       "      <td>GARAGE\\nLIABILITY</td>\n",
       "      <td>GARAGE LIABILITY -\\nBI</td>\n",
       "      <td>4720162839</td>\n",
       "      <td>20240426</td>\n",
       "      <td>Opened</td>\n",
       "      <td>Apple Brett</td>\n",
       "      <td>Motorcyclist was riding\\non motorcycle on East...</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$46,500.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$46,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>GENERAL\\nLIABILITY</td>\n",
       "      <td>GARAGE\\nLIABILITY</td>\n",
       "      <td>GARAGE LIABILITY -\\nPD</td>\n",
       "      <td>4720158513</td>\n",
       "      <td>20240229</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown.</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>GENERAL\\nLIABILITY</td>\n",
       "      <td>GARAGE\\nLIABILITY</td>\n",
       "      <td>GARAGE LIABILITY -\\nPD</td>\n",
       "      <td>4720162839</td>\n",
       "      <td>20240426</td>\n",
       "      <td>Opened</td>\n",
       "      <td>Apple Brett</td>\n",
       "      <td>Motorcyclist was riding\\non motorcycle on East...</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$7.95</td>\n",
       "      <td>$5,992.05</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$6,000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>Claim Count:</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Sum:</td>\n",
       "      <td>$20,529.97</td>\n",
       "      <td>$774.99</td>\n",
       "      <td>$62,817.05</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$84,122.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0              Coverage            Sub Coverage         Val ID\\nDescription  \\\n",
       "0            AUTOMOBILE         AUTO\\nLIABILITY        NO FAULT BI\\nMEDICAL   \n",
       "1            AUTOMOBILE         AUTO\\nLIABILITY        NO FAULT BI\\nMEDICAL   \n",
       "2            AUTOMOBILE  AUTO\\nPHYSICAL\\nDAMAGE                   COLLISION   \n",
       "3            AUTOMOBILE  AUTO\\nPHYSICAL\\nDAMAGE                   COLLISION   \n",
       "4            AUTOMOBILE  AUTO\\nPHYSICAL\\nDAMAGE  NEW VEHICLE\\nCOMPREHENSIVE   \n",
       "..                  ...                     ...                         ...   \n",
       "117          AUTOMOBILE  AUTO\\nPHYSICAL\\nDAMAGE       SERVICE VEHICLE\\nCOMP   \n",
       "118  GENERAL\\nLIABILITY       GARAGE\\nLIABILITY      GARAGE LIABILITY -\\nBI   \n",
       "119  GENERAL\\nLIABILITY       GARAGE\\nLIABILITY      GARAGE LIABILITY -\\nPD   \n",
       "120  GENERAL\\nLIABILITY       GARAGE\\nLIABILITY      GARAGE LIABILITY -\\nPD   \n",
       "121                                Claim Count:                               \n",
       "\n",
       "0   Claim\\nNumber Date of\\nLoss  Status           Claimant\\nName  \\\n",
       "0      4340183468      20211116  Opened      Jackson-Smith\\nIris   \n",
       "1      4340183468      20211116  Opened             Smith Edward   \n",
       "2      1340152292      20211116  Closed         Westbury\\nToyota   \n",
       "3      1510183901      20220326  Closed         Westbury\\nToyota   \n",
       "4      1510178306      20211212  Closed         Westbury\\nToyota   \n",
       "..            ...           ...     ...                      ...   \n",
       "117    1510207883      20231127  Closed  Len Stoler\\nAutomotiv e   \n",
       "118    4720162839      20240426  Opened              Apple Brett   \n",
       "119    4720158513      20240229  Closed                  Unknown   \n",
       "120    4720162839      20240426  Opened              Apple Brett   \n",
       "121             6                                                  \n",
       "\n",
       "0                                   Accident Narrative Paid\\nIndemnity  \\\n",
       "0    \"PIP LOSS\" IV driver\\nexited the parked IV\\nwh...           $0.00   \n",
       "1    \"PIP LOSS\" IV driver\\nexited the parked IV\\nwh...      $46,264.65   \n",
       "2    IV driver exited the\\nparked IV when an OV\\nth...      $34,815.60   \n",
       "3                                            V2 r/e V1       $6,489.19   \n",
       "4    suspect came on lot\\nand lit two vehicles on\\n...      $44,050.82   \n",
       "..                                                 ...             ...   \n",
       "117                      water leak in loaner\\nvehicle       $2,321.21   \n",
       "118  Motorcyclist was riding\\non motorcycle on East...           $0.00   \n",
       "119                                           Unknown.           $0.00   \n",
       "120  Motorcyclist was riding\\non motorcycle on East...           $0.00   \n",
       "121                                               Sum:      $20,529.97   \n",
       "\n",
       "0   Paid\\nExpense Reserves\\nTotal Claim\\nRecovery     Net\\nIncurred  \n",
       "0       $2,926.89           $0.00           $0.00  Total\\n$2,926.89  \n",
       "1       $4,063.15       $7,990.94           $0.00        $58,318.74  \n",
       "2         $165.30           $0.00      ##########         $6,879.49  \n",
       "3         $131.85           $0.00           $0.00         $6,621.04  \n",
       "4         $132.50           $0.00           $0.00        $44,183.32  \n",
       "..            ...             ...             ...               ...  \n",
       "117       $125.00           $0.00           $0.00         $2,446.21  \n",
       "118         $0.00      $46,500.00           $0.00        $46,500.00  \n",
       "119         $0.00           $0.00           $0.00             $0.00  \n",
       "120         $7.95       $5,992.05           $0.00         $6,000.00  \n",
       "121       $774.99      $62,817.05           $0.00        $84,122.01  \n",
       "\n",
       "[122 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfs_by_column_count[13])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
