{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file containing the Textract output\n",
    "with open('textract_output.json', 'r') as json_file:\n",
    "    textract_response = json.load(json_file)\n",
    "\n",
    "def extract_tables_from_json(response):\n",
    "    # Create a dictionary to map Block Ids to Blocks for faster lookup\n",
    "    block_map = {block['Id']: block for block in response['Blocks']}\n",
    "    \n",
    "    tables = []\n",
    "    for block in response['Blocks']:\n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            table = {}\n",
    "            for relationship in block.get('Relationships', []):\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    cell_ids = relationship['Ids']\n",
    "                    for cell_id in cell_ids:\n",
    "                        cell_block = block_map.get(cell_id)\n",
    "                        if cell_block and cell_block['BlockType'] == 'CELL':\n",
    "                            row = cell_block['RowIndex']\n",
    "                            col = cell_block['ColumnIndex']\n",
    "                            cell_text = ''\n",
    "                            \n",
    "                            # Get the text from the WORD blocks inside the CELL block\n",
    "                            for rel in cell_block.get('Relationships', []):\n",
    "                                if rel['Type'] == 'CHILD':\n",
    "                                    for word_id in rel['Ids']:\n",
    "                                        word_block = block_map.get(word_id)\n",
    "                                        if word_block and word_block['BlockType'] == 'WORD':\n",
    "                                            cell_text += word_block['Text'] + ' '\n",
    "                            \n",
    "                            # Add the cell text to the table dictionary\n",
    "                            if row not in table:\n",
    "                                table[row] = {}\n",
    "                            table[row][col] = cell_text.strip()\n",
    "            tables.append(table)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function to extract tables\n",
    "tables = extract_tables_from_json(textract_response)\n",
    "\n",
    "# Display the extracted table data as DataFrames for better readability\n",
    "for table in tables:\n",
    "    # Convert the extracted table into a DataFrame\n",
    "    df = pd.DataFrame.from_dict(table, orient='index')\n",
    "    #print(df)\n",
    "\n",
    "# Set the first row as the column headers\n",
    "df.columns = df.iloc[0]  # Use the first row as header\n",
    "df = df.drop(df.index[0])  # Drop the first row from the DataFrame\n",
    "\n",
    "# Remove duplicate claims\n",
    "df = df.drop_duplicates(subset='Claim Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAIError, RateLimitError\n",
    "from langchain import HuggingFaceHub, PromptTemplate\n",
    "import tiktoken  # Make sure to install this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key from Environment Variable (if still using)\n",
    "huggingfacehub_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# Initialize LLM with Hugging Face Model\n",
    "llm = HuggingFaceHub(repo_id=\"flan-t5\", model_kwargs={\"temperature\": 0.7}, huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# Define prompt to instruct the LLM\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"data\"],\n",
    "    template=\"\"\"\n",
    "    You are a domain expert in insurance claims. \n",
    "    Given the following claims data, please remove all claims not relevant to open dealer lots and involve third-party coverage.\n",
    "    Return the Claim Number of the remaining claims along with a short description of why the claim was included. \n",
    "    Do not include any of the input text or the prompt in your response..\n",
    "    {data}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Calculate prompt tokens\n",
    "prompt_tokens = count_tokens(prompt.template)\n",
    "    \n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    # Use the appropriate encoding for the model\n",
    "    enc = tiktoken.encoding_for_model(\"flan-t5\")  # Change as needed\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Function to batch the DataFrame\n",
    "def batch_dataframe(df, max_tokens):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = row.to_string(index=False, header=False)  # Convert row to string\n",
    "        row_tokens = count_tokens(row_str)\n",
    "\n",
    "        # Check if adding this row exceeds the limit\n",
    "        if current_tokens + row_tokens + prompt_tokens > max_tokens:\n",
    "            if current_batch:  # If there's already data in the current batch\n",
    "                batches.append(pd.DataFrame(current_batch))\n",
    "                current_batch = []  # Reset current batch\n",
    "                current_tokens = 0\n",
    "\n",
    "        current_batch.append(row)\n",
    "        current_tokens += row_tokens\n",
    "\n",
    "    if current_batch:  # Add any remaining data\n",
    "        batches.append(pd.DataFrame(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Convert the entire DataFrame into batches\n",
    "max_tokens = 1024  # Adjust based on your model's limit\n",
    "df_batches = batch_dataframe(df, max_tokens)\n",
    "\n",
    "# Process each batch\n",
    "for i, batch in enumerate(df_batches):\n",
    "    data_string = batch.to_string(index=False, header=True)\n",
    "    \n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Format and send the prompt to the model\n",
    "            response = llm(prompt.format(data=data_string))\n",
    "            #print(f\"Response received successfully for batch {i + 1}:\")\n",
    "            print(response)  # Print the LLM's response\n",
    "            break  # Exit loop if the request is successful\n",
    "        except RateLimitError:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break  # Exit the loop on other OpenAI errors\n",
    "    else:\n",
    "        print(\"Max retries reached. Could not complete the request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8edd1e81b3471eab3ec8ab9a1b8c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  20%|##        | 62.9M/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f9fa642701429b89f039d28f77fce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mYou are a domain expert in insurance claims. \u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mGiven the following claims data, please remove all claims not relevant to open dealer lots and involve third-party coverage.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mCLAIM DATA GOES HERE\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Process each batch\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdf_batches\u001b[49m):\n\u001b[1;32m     29\u001b[0m     data_string \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the data string from the batch\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Create the full prompt with data included\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_batches' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for flan-t5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown encoding flan-t5.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.8.0 (are you on latest?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Calculate prompt tokens and convert the entire DataFrame into batches\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcount_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m  \u001b[38;5;66;03m# Adjust based on your model's limit\u001b[39;00m\n\u001b[1;32m     78\u001b[0m df_batches \u001b[38;5;241m=\u001b[39m batch_dataframe(df, max_tokens, prompt_tokens)\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mcount_tokens\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_tokens\u001b[39m(text):\n\u001b[0;32m---> 19\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflan-t5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the appropriate encoding for the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(enc\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "File \u001b[0;32m~/UW-Madison_Undergraduate/Understory/venv/lib/python3.12/site-packages/tiktoken/registry.py:79\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ENCODING_CONSTRUCTORS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENCODING_CONSTRUCTORS:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiktoken version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (are you on latest?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[1;32m     86\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor())\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown encoding flan-t5.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.8.0 (are you on latest?)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model for flan-t5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Define the prompt (your task description and data)\n",
    "prompt = \"\"\"\n",
    "    You are a domain expert in insurance claims. \n",
    "    Given the following claims data, remove all claims either not relevant to open dealer lots or involve third-party coverage or both.\n",
    "    Return the Claim Number of the remaining claims along with a short description of why the claim was included.\n",
    "    CLAIM DATA GOES HERE\n",
    "    \"\"\"\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    enc = tiktoken.get_encoding(\"flan-t5\")  # Use the appropriate encoding for the model\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "# Function to batch the DataFrame\n",
    "def batch_dataframe(df, max_tokens, prompt_tokens):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        row_str = row.to_string(index=False, header=False)  # Convert row to string\n",
    "        row_tokens = count_tokens(row_str)\n",
    "\n",
    "        # Check if adding this row exceeds the limit\n",
    "        if current_tokens + row_tokens + prompt_tokens > max_tokens:\n",
    "            if current_batch:  # If there's already data in the current batch\n",
    "                batches.append(pd.DataFrame(current_batch))\n",
    "                current_batch = []  # Reset current batch\n",
    "                current_tokens = 0\n",
    "\n",
    "        current_batch.append(row)\n",
    "        current_tokens += row_tokens\n",
    "\n",
    "    if current_batch:  # Add any remaining data\n",
    "        batches.append(pd.DataFrame(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Generate responses for each batch\n",
    "def process_batches(batches, prompt, model, tokenizer, max_length=200):\n",
    "    responses = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        data_string = batch.to_string(index=False, header=True)  # Create the data string from the batch\n",
    "        \n",
    "        # Create the full prompt with data included\n",
    "        full_prompt = prompt.replace(\"CLAIM DATA GOES HERE\", data_string)  # Replace placeholder with data\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(full_prompt, model, tokenizer, max_length)\n",
    "            responses.append((i + 1, response))  # Append the batch number and response\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i + 1}: {e}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=200):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "\n",
    "    # Decode the generated tokens to get the output text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Calculate prompt tokens and convert the entire DataFrame into batches\n",
    "prompt_tokens = count_tokens(prompt)\n",
    "max_tokens = 512  # Adjust based on your model's limit\n",
    "df_batches = batch_dataframe(df, max_tokens, prompt_tokens)\n",
    "\n",
    "# Process the batches and get responses\n",
    "responses = process_batches(df_batches, prompt, model, tokenizer)\n",
    "\n",
    "# Print or handle the responses\n",
    "for batch_num, response in responses:\n",
    "    print(f\"Response for batch {batch_num}:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a3fc2e6d124959849c2059c94d3967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 1: The current model class (BertModel) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the tokenizer and model for FLAN-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModel.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "# Define the prompt (your task description and data)\n",
    "prompt_template = \"\"\"\n",
    "You are a domain expert in insurance claims. \n",
    "Given the following claims data, remove all claims either not relevant to open dealer lots or involve third-party coverage or both.\n",
    "Return the Claim Number of the remaining claims along with a short description of why the claim was included.\n",
    "CLAIM DATA GOES HERE\n",
    "\"\"\"\n",
    "\n",
    "# Function to count tokens using Hugging Face tokenizer\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return tokens.size(1)  # Return the token count\n",
    "\n",
    "# Function to batch the DataFrame\n",
    "def batch_dataframe(df, max_tokens, prompt_tokens):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_str = row.to_string(index=False, header=False)  # Convert row to string\n",
    "        row_tokens = count_tokens(row_str)\n",
    "\n",
    "        # Check if adding this row exceeds the limit\n",
    "        if current_tokens + row_tokens + prompt_tokens > max_tokens:\n",
    "            if current_batch:  # If there's already data in the current batch\n",
    "                batches.append(pd.DataFrame(current_batch))\n",
    "                current_batch = []  # Reset current batch\n",
    "                current_tokens = 0\n",
    "\n",
    "        current_batch.append(row)\n",
    "        current_tokens += row_tokens\n",
    "\n",
    "    if current_batch:  # Add any remaining data\n",
    "        batches.append(pd.DataFrame(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Generate responses for each batch\n",
    "def process_batches(batches, prompt_template, model, tokenizer, max_length=200):\n",
    "    responses = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        data_string = batch.to_string(index=False, header=True)  # Create the data string from the batch\n",
    "        \n",
    "        # Create the full prompt with data included\n",
    "        full_prompt = prompt_template.replace(\"CLAIM DATA GOES HERE\", data_string)  # Replace placeholder with data\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(full_prompt, model, tokenizer, max_length)\n",
    "            responses.append((i + 1, response))  # Append the batch number and response\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i + 1}: {e}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt, model, tokenizer, max_length=200):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "\n",
    "    # Decode the generated tokens to get the output text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example DataFrame (replace this with your actual claims data)\n",
    "data = {'Claim Number': [101, 102, 103],\n",
    "        'Claim Description': [\"Open lot damage due to rain\", \n",
    "                              \"Third-party liability claim for auto accident\", \n",
    "                              \"Open lot damage from hail\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate prompt tokens and convert the entire DataFrame into batches\n",
    "prompt_tokens = count_tokens(prompt_template)\n",
    "max_tokens = 512  # Adjust based on your model's limit\n",
    "df_batches = batch_dataframe(df, max_tokens, prompt_tokens)\n",
    "\n",
    "# Process the batches and get responses\n",
    "responses = process_batches(df_batches, prompt_template, model, tokenizer)\n",
    "\n",
    "# Print or handle the responses\n",
    "for batch_num, response in responses:\n",
    "    print(f\"Response for batch {batch_num}:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5fc3a238adc25dec0d5b2079827dca283ce2d8f5922d8045d64c5203803846b"
  },
  "kernelspec": {
   "display_name": "Python Understory venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
