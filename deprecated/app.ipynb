{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test File to Explore AWS Textract Parsing Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary imports in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAIError, RateLimitError\n",
    "from langchain import HuggingFaceHub, PromptTemplate\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Call to Retrieve Entire Document Analysis\n",
    "- Requires a job ID from previous call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobId': 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3'}\n",
      "{'JobId': 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3', 'NextToken': 'VDZS/xOfdp0zpWvvVE4/0e6UIUK7SSlFUKY6eu2GHYGA4X8D9jHW0tzK6okH4PYcKiWliGAJtET/fEHnjdPnQGpwniOJY+2GD0i/w6Gp/Or9LFY3byZdaYk+BgiAynDB4lAGx3s='}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Textract client\n",
    "textract = boto3.client('textract', region_name='us-east-1')\n",
    "\n",
    "def get_all_document_analysis(job_id):\n",
    "    results = []\n",
    "    next_token = None\n",
    "\n",
    "    while True:\n",
    "        # Prepare parameters for the API call\n",
    "        params = {\n",
    "            'JobId': job_id\n",
    "        }\n",
    "        \n",
    "        if next_token:\n",
    "            params['NextToken'] = next_token  # Include NextToken if available\n",
    "\n",
    "        # Call the Textract API\n",
    "        print(params)\n",
    "        response = textract.get_document_analysis(**params)\n",
    "\n",
    "        # Append the blocks to results\n",
    "        results.extend(response.get('Blocks', []))\n",
    "\n",
    "        # Check for NextToken in the response\n",
    "        next_token = response.get('NextToken')\n",
    "\n",
    "        # Break if there's no more pages\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "job_id = 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3'\n",
    "all_results = get_all_document_analysis(job_id)\n",
    "\n",
    "with open('textract_output_next_page.json', 'w') as json_file:\n",
    "    json.dump(all_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file containing the Textract output\n",
    "with open('textract_output.json', 'r') as json_file:\n",
    "    textract_response = json.load(json_file)\n",
    "\n",
    "def extract_tables_from_json(response):\n",
    "    # Create a dictionary to map Block Ids to Blocks for faster lookup\n",
    "    block_map = {block['Id']: block for block in response['Blocks']}\n",
    "    \n",
    "    tables = []\n",
    "    current_table = {}\n",
    "    current_page = 1\n",
    "    \n",
    "    for block in response['Blocks']:\n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            table = {}\n",
    "            for relationship in block.get('Relationships', []):\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    cell_ids = relationship['Ids']\n",
    "                    for cell_id in cell_ids:\n",
    "                        cell_block = block_map.get(cell_id)\n",
    "                        if cell_block and cell_block['BlockType'] == 'CELL':\n",
    "                            row = cell_block['RowIndex']\n",
    "                            col = cell_block['ColumnIndex']\n",
    "                            cell_text = ''\n",
    "                            \n",
    "                            # Get the text from the WORD blocks inside the CELL block\n",
    "                            for rel in cell_block.get('Relationships', []):\n",
    "                                if rel['Type'] == 'CHILD':\n",
    "                                    for word_id in rel['Ids']:\n",
    "                                        word_block = block_map.get(word_id)\n",
    "                                        if word_block and word_block['BlockType'] == 'WORD':\n",
    "                                            cell_text += word_block['Text'] + ' '\n",
    "                            \n",
    "                            # Add the cell text to the table dictionary\n",
    "                            if row not in table:\n",
    "                                table[row] = {}\n",
    "                            table[row][col] = cell_text.strip()\n",
    "            \n",
    "            # Check if the table belongs to the current page\n",
    "            if block.get('Page') == current_page:\n",
    "                # Merge current table with the new one if it's on the same page\n",
    "                current_table = merge_tables(current_table, table)\n",
    "            else:\n",
    "                # If page number changes, push the current table to the list and reset\n",
    "                if current_table:\n",
    "                    tables.append(current_table)\n",
    "                current_table = table\n",
    "                current_page = block.get('Page', current_page)\n",
    "\n",
    "    # Append the last table after iteration\n",
    "    if current_table:\n",
    "        tables.append(current_table)\n",
    "        \n",
    "    return tables\n",
    "\n",
    "def merge_tables(existing_table, new_table):\n",
    "    \"\"\"Merges two tables by appending rows from the new table to the existing table.\"\"\"\n",
    "    merged_table = existing_table.copy()\n",
    "    \n",
    "    for row, cols in new_table.items():\n",
    "        if row in merged_table:\n",
    "            # If row exists, merge column values\n",
    "            merged_table[row].update(cols)\n",
    "        else:\n",
    "            # Add the new row if not present\n",
    "            merged_table[row] = cols\n",
    "            \n",
    "    return merged_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file containing the Textract output\n",
    "with open('textract_output.json', 'r') as json_file:\n",
    "    textract_response = json.load(json_file)\n",
    "\n",
    "def extract_tables_from_json(response):\n",
    "    # Create a dictionary to map Block Ids to Blocks for faster lookup\n",
    "    block_map = {block['Id']: block for block in response['Blocks']}\n",
    "    \n",
    "    tables = []\n",
    "    for block in response['Blocks']:\n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            table = {}\n",
    "            for relationship in block.get('Relationships', []):\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    cell_ids = relationship['Ids']\n",
    "                    for cell_id in cell_ids:\n",
    "                        cell_block = block_map.get(cell_id)\n",
    "                        if cell_block and cell_block['BlockType'] == 'CELL':\n",
    "                            row = cell_block['RowIndex']\n",
    "                            col = cell_block['ColumnIndex']\n",
    "                            cell_text = ''\n",
    "                            \n",
    "                            # Get the text from the WORD blocks inside the CELL block\n",
    "                            for rel in cell_block.get('Relationships', []):\n",
    "                                if rel['Type'] == 'CHILD':\n",
    "                                    for word_id in rel['Ids']:\n",
    "                                        word_block = block_map.get(word_id)\n",
    "                                        if word_block and word_block['BlockType'] == 'WORD':\n",
    "                                            cell_text += word_block['Text'] + ' '\n",
    "                            \n",
    "                            # Add the cell text to the table dictionary\n",
    "                            if row not in table:\n",
    "                                table[row] = {}\n",
    "                            table[row][col] = cell_text.strip()\n",
    "            tables.append(table)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function to extract tables\n",
    "tables = extract_tables_from_json(textract_response)\n",
    "\n",
    "# Display the extracted table data as DataFrames for better readability\n",
    "for table in tables:\n",
    "    # Convert the extracted table into a DataFrame\n",
    "    df = pd.DataFrame.from_dict(table, orient='index')\n",
    "    #print(df)\n",
    "\n",
    "# Set the first row as the column headers\n",
    "df.columns = df.iloc[0]  # Use the first row as header\n",
    "df = df.drop(df.index[0])  # Drop the first row from the DataFrame\n",
    "\n",
    "# Remove duplicate claims\n",
    "df = df.drop_duplicates(subset='Claim Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>Claim Number</th>\n",
       "      <th>Policy Insured Code</th>\n",
       "      <th>Dealer / Lessee Name</th>\n",
       "      <th>VIN</th>\n",
       "      <th>Subline</th>\n",
       "      <th>State / Province</th>\n",
       "      <th>Cause of Loss</th>\n",
       "      <th>Loss Date</th>\n",
       "      <th>Claim Close Date</th>\n",
       "      <th>Units</th>\n",
       "      <th>Indemnity Payments</th>\n",
       "      <th>Indemnity Expenses</th>\n",
       "      <th>Net Insurance Recoveries</th>\n",
       "      <th>Indemnity Payments Net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150123008611</td>\n",
       "      <td>EKA119</td>\n",
       "      <td>ANDERSON FORD OF ST JOSEP</td>\n",
       "      <td>2LMDU88C77BJ14427</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>Collision</td>\n",
       "      <td>01/23/2015</td>\n",
       "      <td>02/19/2021</td>\n",
       "      <td>0</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161216008812</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>2LMHJ5AT5CBL55030</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Collision</td>\n",
       "      <td>12/16/2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$607.50</td>\n",
       "      <td>$2,002.46</td>\n",
       "      <td>$607.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>190806002768</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>MULTI</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Wind</td>\n",
       "      <td>08/06/2019</td>\n",
       "      <td>12/23/2020</td>\n",
       "      <td>8</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$1,677.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>190927007769</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>2FMPK4J91KBB61371</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Theft</td>\n",
       "      <td>09/27/2019</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$586.07</td>\n",
       "      <td>$35,200.00</td>\n",
       "      <td>$586.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200118002308</td>\n",
       "      <td>EKA119</td>\n",
       "      <td>ANDERSON FORD OF ST JOSEP</td>\n",
       "      <td>1FT8W3DT3HEC08556</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>Theft</td>\n",
       "      <td>02/18/2017</td>\n",
       "      <td>09/17/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$27,750.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200206005764</td>\n",
       "      <td>EKA894</td>\n",
       "      <td>ANDERSON FORD OF LINCOLN / ANDERSON LINCOLN OF...</td>\n",
       "      <td>JN8AT2MV6HW016228</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Collision</td>\n",
       "      <td>02/06/2020</td>\n",
       "      <td>02/11/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200218006998</td>\n",
       "      <td>EKA965</td>\n",
       "      <td>ANDERSON KIA</td>\n",
       "      <td>5XYPHDA52LG647197</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>All Other (non WX)</td>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>04/07/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "1   Claim Number Policy Insured Code  \\\n",
       "2   150123008611              EKA119   \n",
       "4   161216008812              EKAA4G   \n",
       "6   190806002768              EKAA4G   \n",
       "8   190927007769              EKAA4G   \n",
       "10  200118002308              EKA119   \n",
       "12  200206005764              EKA894   \n",
       "14  200218006998              EKA965   \n",
       "\n",
       "1                                Dealer / Lessee Name                VIN  \\\n",
       "2                           ANDERSON FORD OF ST JOSEP  2LMDU88C77BJ14427   \n",
       "4                       ANDERSON FORD OF GRAND ISLAND  2LMHJ5AT5CBL55030   \n",
       "6                       ANDERSON FORD OF GRAND ISLAND              MULTI   \n",
       "8                       ANDERSON FORD OF GRAND ISLAND  2FMPK4J91KBB61371   \n",
       "10                          ANDERSON FORD OF ST JOSEP  1FT8W3DT3HEC08556   \n",
       "12  ANDERSON FORD OF LINCOLN / ANDERSON LINCOLN OF...  JN8AT2MV6HW016228   \n",
       "14                                       ANDERSON KIA  5XYPHDA52LG647197   \n",
       "\n",
       "1  Subline State / Province       Cause of Loss   Loss Date Claim Close Date  \\\n",
       "2   FCNAUS               MO           Collision  01/23/2015       02/19/2021   \n",
       "4   FCNAUS               NE           Collision  12/16/2016                    \n",
       "6   FCNAUS               NE                Wind  08/06/2019       12/23/2020   \n",
       "8   FCNAUS               NE               Theft  09/27/2019                    \n",
       "10  FCNAUS               MO               Theft  02/18/2017       09/17/2020   \n",
       "12  FCNAUS               NE           Collision  02/06/2020       02/11/2020   \n",
       "14  FCNAUS               MO  All Other (non WX)  02/18/2020       04/07/2020   \n",
       "\n",
       "1  Units Indemnity Payments Indemnity Expenses Net Insurance Recoveries  \\\n",
       "2      0              $0.00              $0.00                    $0.00   \n",
       "4      0              $0.00            $607.50                $2,002.46   \n",
       "6      8              $0.00              $0.00                $1,677.00   \n",
       "8      1              $0.00            $586.07               $35,200.00   \n",
       "10     1              $0.00              $0.00               $27,750.00   \n",
       "12     1              $0.00              $0.00                    $0.00   \n",
       "14     1              $0.00              $0.00                    $0.00   \n",
       "\n",
       "1  Indemnity Payments Net  \n",
       "2                   $0.00  \n",
       "4                 $607.50  \n",
       "6                   $0.00  \n",
       "8                 $586.07  \n",
       "10                  $0.00  \n",
       "12                  $0.00  \n",
       "14                  $0.00  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161216008812 EKAA4G ANDERSON FORD OF GRAND ISLAND 2LMHJ5AT5CBL55030 FCNAUS NE Collision 12/16/2016  0 $0.00 $607.50 $2,002.46 $607.50'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161216008812 EKAA4G ANDERSON FORD OF GRAND ISLAND 2LMHJ5AT5CBL55030 FCNAUS NE Collision 12/16/2016  0    '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern to match prices\n",
    "pattern = r'\\$([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(\\.[0-9]{2})?'\n",
    "\n",
    "# Removing prices from the text column\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for batch 1:\n",
      "['Relevant']\n",
      "Response for batch 2:\n",
      "['Relevant']\n",
      "Response for batch 3:\n",
      "['Relevant']\n",
      "Response for batch 4:\n",
      "['Relevant']\n",
      "Response for batch 5:\n",
      "['Relevant']\n",
      "Response for batch 6:\n",
      "['Relevant']\n",
      "Response for batch 7:\n",
      "['Relevant']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)  # 2 labels: relevant or not\n",
    "\n",
    "# Define the prompt (your task description and data)\n",
    "prompt_template = \"\"\"\n",
    "Given the following claims data, remove all claims not relevant to open dealer lot coverage or that involve third-party coverage.\n",
    "Return the Claim Number of the remaining claims along with a short description of why each was included.\n",
    "CLAIM DATA GOES HERE\n",
    "\"\"\"\n",
    "\n",
    "# Function to count tokens using Hugging Face tokenizer\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return tokens.size(1)  # Return the token count\n",
    "\n",
    "# Function to batch the DataFrame\n",
    "def batch_dataframe(df, max_tokens, prompt_tokens):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_str = row.to_string(index=False, header=False)  # Convert row to string\n",
    "        row_tokens = count_tokens(row_str)\n",
    "\n",
    "        # Check if adding this row exceeds the limit\n",
    "        if current_tokens + row_tokens + prompt_tokens > max_tokens:\n",
    "            if current_batch:  # If there's already data in the current batch\n",
    "                batches.append(pd.DataFrame(current_batch))\n",
    "                current_batch = []  # Reset current batch\n",
    "                current_tokens = 0\n",
    "\n",
    "        current_batch.append(row)\n",
    "        current_tokens += row_tokens\n",
    "\n",
    "    if current_batch:  # Add any remaining data\n",
    "        batches.append(pd.DataFrame(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Generate responses for each batch\n",
    "def process_batches(batches, prompt_template, model, tokenizer, max_length=200):\n",
    "    responses = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        data_string = batch.to_string(index=False, header=True)  # Create the data string from the batch\n",
    "        \n",
    "        # Create the full prompt with data included\n",
    "        full_prompt = prompt_template.replace(\"CLAIM DATA GOES HERE\", data_string)  # Replace placeholder with data\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(full_prompt, model, tokenizer, max_length)\n",
    "            responses.append((i + 1, response))  # Append the batch number and response\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i + 1}: {e}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt, model, tokenizer, max_length=200):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "\n",
    "    # Decode the generated tokens to get the output text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Calculate prompt tokens and convert the entire DataFrame into batches\n",
    "prompt_tokens = count_tokens(prompt_template)\n",
    "max_tokens = 512  # Adjust based on model limit\n",
    "df_batches = batch_dataframe(df, max_tokens, prompt_tokens)\n",
    "\n",
    "# Process the batches and get responses\n",
    "responses = process_batches(df_batches, prompt_template, model, tokenizer)\n",
    "\n",
    "# Print or handle the responses\n",
    "for batch_num, response in responses:\n",
    "    print(f\"Response for batch {batch_num}:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at llmware/industry-bert-insurance-v0.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "def create_prompt(claim):\n",
    "    return f\"Evaluate the following claim for relevance to open dealer lot coverage excluding third parties:{claim}\\nIs this claim relevant? (yes/no)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim 150123008611 Logits: tensor([[ 0.1962, -0.2636]]), Probabilities: tensor([[0.6130, 0.3870]])\n",
      "Claim 161216008812 Logits: tensor([[ 0.1819, -0.2930]]), Probabilities: tensor([[0.6165, 0.3835]])\n",
      "Claim 190806002768 Logits: tensor([[ 0.1818, -0.2586]]), Probabilities: tensor([[0.6084, 0.3916]])\n",
      "Claim 190927007769 Logits: tensor([[ 0.1822, -0.2629]]), Probabilities: tensor([[0.6095, 0.3905]])\n",
      "Claim 200118002308 Logits: tensor([[ 0.2032, -0.2967]]), Probabilities: tensor([[0.6224, 0.3776]])\n",
      "Claim 200206005764 Logits: tensor([[ 0.1700, -0.3021]]), Probabilities: tensor([[0.6159, 0.3841]])\n",
      "Claim 200218006998 Logits: tensor([[ 0.1874, -0.2653]]), Probabilities: tensor([[0.6113, 0.3887]])\n",
      "Filtered Claims:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Classify claims\n",
    "def classify_claims(df):\n",
    "    relevant_claims = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = create_prompt(row)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
    "            \n",
    "            # Print logits and probabilities for debugging\n",
    "            print(f\"Claim {row['Claim Number']} Logits: {logits}, Probabilities: {probabilities}\")\n",
    "\n",
    "            predicted_class = logits.argmax().item()  # Get the predicted class index\n",
    "\n",
    "        # Assuming class index 1 is \"relevant\"\n",
    "        if predicted_class == 1:  # Adjust based on your model's output class index\n",
    "            relevant_claims.append(row)\n",
    "\n",
    "    return pd.DataFrame(relevant_claims)\n",
    "\n",
    "# Get relevant claims\n",
    "relevant_claims_df = classify_claims(df)\n",
    "\n",
    "# Print the filtered claims\n",
    "print(\"Filtered Claims:\")\n",
    "print(relevant_claims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.oauth2 import service_account\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# Prepare variables\n",
    "project_id = 'verdant-cargo-443521-j3'\n",
    "location = 'us'\n",
    "processor_id = 'f2db9bcc34ed8bb5'\n",
    "\n",
    "file_path = '/path/to/local/file/.pdf'\n",
    "mime_type = 'application/pdf'\n",
    "\n",
    "# Load the service account key\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"../secret/verdant-cargo-443521-j3-8d56893a2e2e.json\"\n",
    ")\n",
    "\n",
    "opts = {\n",
    "    \"api_endpoint\" : f\"{location}-documentai.googleapis.com\"\n",
    "}\n",
    "\n",
    "# Configure the processor client (i.e. prepare the endpoint)\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=opts, credentials=credentials)\n",
    "\n",
    "name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "#Open File\n",
    "with open('../classification_model/loss_runs/input/Copy of Stoler of Queens, Inc dba Silver Star Motors - Loss Run - 942024.pdf', 'rb') as pdf_file:\n",
    "    pdf_data = pdf_file.read()\n",
    "\n",
    "# Construct the request\n",
    "raw_document = documentai.RawDocument(content=pdf_data, mime_type=mime_type)\n",
    "\n",
    "request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "\n",
    "# Analyze output\n",
    "result = client.process_document(request=request)\n",
    "\n",
    "document = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown field for Table: visualize",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m document\u001b[38;5;241m.\u001b[39mpages:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m page\u001b[38;5;241m.\u001b[39mtables:\n\u001b[0;32m----> 5\u001b[0m         display(\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m(figsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# #print(table)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# for row in table.header_rows:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m#     row_text = \"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m#     #print(f'{row_text}\\n')\u001b[39;00m\n",
      "File \u001b[0;32m~/UW-Madison_Undergraduate/Professional/Understory/venv/lib/python3.12/site-packages/proto/message.py:877\u001b[0m, in \u001b[0;36mMessage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    875\u001b[0m (key, pb_type) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pb_type_from_key(key)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pb_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, key)\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m pb_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pb, key)\n\u001b[1;32m    881\u001b[0m marshal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mmarshal\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unknown field for Table: visualize"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "print(document)\n",
    "for page in document.pages:\n",
    "    for table in page.tables:\n",
    "        \n",
    "# display several images\n",
    "# decrease size with plt\n",
    "for table in tables:\n",
    "    display(table.visualize(figsize=None))\n",
    "        #print(table)\n",
    "        for row in table.header_rows:\n",
    "            row_text = \"\"\n",
    "            for cell in row.cells:\n",
    "                cell_text = \"\"\n",
    "                for text_segment in cell.layout.text_anchor.text_segments:\n",
    "                    cell_text += document.text[text_segment.start_index:text_segment.end_index]\n",
    "                row_text += cell_text.strip() + ' '\n",
    "                \n",
    "            #print(f'{row_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1 as docai\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "from typing import Iterator, MutableSequence, Optional, Sequence, Tuple, List\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "def table_extraction(\n",
    "    document: docai.Document\n",
    ") -> List[pd.DataFrame]:\n",
    "    '''\n",
    "    Function to extract tables from document AI output\n",
    "    '''\n",
    "    # Extract document text\n",
    "    full_text = document.text\n",
    "    tables_as_dataframes = []\n",
    "\n",
    "    # Iterate over available pages and handle each available table\n",
    "    for page_index, page in enumerate(document.pages):\n",
    "        print(f\"\\n--- Page {page_index + 1} ---\\n\")\n",
    "\n",
    "        for table_index, table in enumerate(page.tables):\n",
    "            print(f\"Table {table_index + 1}:\")\n",
    "            # Extract and print header and body rows\n",
    "            headers = extract_table_rows(table.header_rows, full_text)\n",
    "            body = extract_table_rows(table.body_rows, full_text)\n",
    "\n",
    "            # Use the first header row (if available) as column names\n",
    "            if headers:\n",
    "                df = pd.DataFrame(body, columns=headers[0])\n",
    "            else:\n",
    "                # If no headers, create generic column names\n",
    "                num_columns = max(len(row) for row in body) if body else 0\n",
    "                df = pd.DataFrame(body, columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
    "            \n",
    "            # Append the DataFrame for this table\n",
    "            tables_as_dataframes.append(df)\n",
    "    \n",
    "    return tables_as_dataframes\n",
    "\n",
    "def extract_table_rows(\n",
    "    table_rows: Sequence[docai.Document.Page.Table.TableRow], text: str\n",
    ") -> List[List[str]]:\n",
    "    '''\n",
    "    Extracts rows of text from Document AI table rows\n",
    "    '''\n",
    "    rows = []\n",
    "    for table_row in table_rows:\n",
    "        row_data = []\n",
    "        for cell in table_row.cells:\n",
    "            cell_text = layout_to_text(cell.layout, text).strip()\n",
    "            row_data.append(cell_text)\n",
    "        rows.append(row_data)\n",
    "    return rows\n",
    "\n",
    "def layout_to_text(layout: docai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document\"s text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    return \"\".join(\n",
    "        text[int(segment.start_index) : int(segment.end_index)]\n",
    "        for segment in layout.text_anchor.text_segments\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Page 1 ---\n",
      "\n",
      "Table 1:\n",
      "Table 2:\n"
     ]
    }
   ],
   "source": [
    "dfs = table_extraction(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Number</th>\n",
       "      <th>Insured Name</th>\n",
       "      <th>Cause of\\nLoss</th>\n",
       "      <th>Date of\\nLoss</th>\n",
       "      <th>Description of Accident</th>\n",
       "      <th>Indemnity</th>\n",
       "      <th>Expense</th>\n",
       "      <th>Recovery</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>907885-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba\\nSilver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>7/25/2024</td>\n",
       "      <td>IVD trying to park vehicle\\nin stacker lift th...</td>\n",
       "      <td>17,143.90</td>\n",
       "      <td>240.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907814-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba\\nSilver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>6/21/2024</td>\n",
       "      <td>Rental was involved in\\ncollision while being ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>400.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>907812-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba\\nSilver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>4/23/2024</td>\n",
       "      <td>Customer was involved in a collision while in ...</td>\n",
       "      <td>6,262.12</td>\n",
       "      <td>1,300.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Claim Number                                   Insured Name Cause of\\nLoss  \\\n",
       "0    907885-CB  Stoler of Queens, Inc dba\\nSilver Star Motors      Collision   \n",
       "1    907814-CB  Stoler of Queens, Inc dba\\nSilver Star Motors      Collision   \n",
       "2    907812-CB  Stoler of Queens, Inc dba\\nSilver Star Motors      Collision   \n",
       "\n",
       "  Date of\\nLoss                            Description of Accident  Indemnity  \\\n",
       "0     7/25/2024  IVD trying to park vehicle\\nin stacker lift th...  17,143.90   \n",
       "1     6/21/2024  Rental was involved in\\ncollision while being ...       0.00   \n",
       "2     4/23/2024  Customer was involved in a collision while in ...   6,262.12   \n",
       "\n",
       "    Expense Recovery  Status  \n",
       "0    240.00     0.00  Closed  \n",
       "1    400.00     0.00  Closed  \n",
       "2  1,300.00     0.00    Open  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gmft_pymupdf import PyMuPDFDocument\n",
    "from gmft.auto import AutoTableDetector, AutoTableFormatter, AutoFormatConfig\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def gmft_process_pdf(file_path: str) -> List[pd.DataFrame]:\n",
    "    # Open Document\n",
    "    doc = PyMuPDFDocument(file_path)\n",
    "\n",
    "    # Initialize Detector and Formatter\n",
    "    detector = AutoTableDetector()\n",
    "    formatter = AutoTableFormatter()\n",
    "    config = AutoFormatConfig(\n",
    "        semantic_spanning_cells=True,  # If tables have merged cells\n",
    "        verbosity=3\n",
    "    )\n",
    "\n",
    "    # Extract Tables\n",
    "    tables = []\n",
    "    for page in doc:\n",
    "        tables += detector.extract(page)\n",
    "    \n",
    "    # Convert Tables to Dataframes\n",
    "    dfs = []\n",
    "    for table in tables:\n",
    "        ft = formatter.extract(table)\n",
    "        dfs.append(ft.df()) # formatter's tables automatically uses settings of config\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = gmft_process_pdf('../classification_model/loss_runs/input/Copy of Stoler of Queens, Inc dba Silver Star Motors - Loss Run - 942024.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Number</th>\n",
       "      <th>Insured Name</th>\n",
       "      <th>Cause of Loss</th>\n",
       "      <th>Date of Loss</th>\n",
       "      <th>Description of Accident</th>\n",
       "      <th>Indemnity</th>\n",
       "      <th>Expense</th>\n",
       "      <th>Recovery</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>907885-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba Silver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>7/25/2024</td>\n",
       "      <td>IVD trying to park vehicle in stacker lift tha...</td>\n",
       "      <td>17,143.90</td>\n",
       "      <td>240.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907814-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba Silver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>6/21/2024</td>\n",
       "      <td>Rental was involved in collision while being u...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>400.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>907812-CB</td>\n",
       "      <td>Stoler of Queens, Inc dba Silver Star Motors</td>\n",
       "      <td>Collision</td>\n",
       "      <td>4/23/2024</td>\n",
       "      <td>Customer was involved in a collision while in ...</td>\n",
       "      <td>6,262.12</td>\n",
       "      <td>1,300.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Claim Number                                  Insured Name Cause of Loss  \\\n",
       "0    907885-CB  Stoler of Queens, Inc dba Silver Star Motors     Collision   \n",
       "1    907814-CB  Stoler of Queens, Inc dba Silver Star Motors     Collision   \n",
       "2    907812-CB  Stoler of Queens, Inc dba Silver Star Motors     Collision   \n",
       "\n",
       "  Date of Loss                            Description of Accident  Indemnity  \\\n",
       "0    7/25/2024  IVD trying to park vehicle in stacker lift tha...  17,143.90   \n",
       "1    6/21/2024  Rental was involved in collision while being u...       0.00   \n",
       "2    4/23/2024  Customer was involved in a collision while in ...   6,262.12   \n",
       "\n",
       "    Expense Recovery  Status  \n",
       "0    240.00     0.00  Closed  \n",
       "1    400.00     0.00  Closed  \n",
       "2  1,300.00     0.00    Open  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5fc3a238adc25dec0d5b2079827dca283ce2d8f5922d8045d64c5203803846b"
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
