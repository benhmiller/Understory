{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test File to Explore AWS Textract Parsing Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary imports in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAIError, RateLimitError\n",
    "from langchain import HuggingFaceHub, PromptTemplate\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Call to Retrieve Entire Document Analysis\n",
    "- Requires a job ID from previous call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobId': 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3'}\n",
      "{'JobId': 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3', 'NextToken': 'VDZS/xOfdp0zpWvvVE4/0e6UIUK7SSlFUKY6eu2GHYGA4X8D9jHW0tzK6okH4PYcKiWliGAJtET/fEHnjdPnQGpwniOJY+2GD0i/w6Gp/Or9LFY3byZdaYk+BgiAynDB4lAGx3s='}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Textract client\n",
    "textract = boto3.client('textract', region_name='us-east-1')\n",
    "\n",
    "def get_all_document_analysis(job_id):\n",
    "    results = []\n",
    "    next_token = None\n",
    "\n",
    "    while True:\n",
    "        # Prepare parameters for the API call\n",
    "        params = {\n",
    "            'JobId': job_id\n",
    "        }\n",
    "        \n",
    "        if next_token:\n",
    "            params['NextToken'] = next_token  # Include NextToken if available\n",
    "\n",
    "        # Call the Textract API\n",
    "        print(params)\n",
    "        response = textract.get_document_analysis(**params)\n",
    "\n",
    "        # Append the blocks to results\n",
    "        results.extend(response.get('Blocks', []))\n",
    "\n",
    "        # Check for NextToken in the response\n",
    "        next_token = response.get('NextToken')\n",
    "\n",
    "        # Break if there's no more pages\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "job_id = 'a60cf021227e3eb17e2288f6c104f782d0a36cb199f99f46cfefc813224640e3'\n",
    "all_results = get_all_document_analysis(job_id)\n",
    "\n",
    "with open('textract_output_next_page.json', 'w') as json_file:\n",
    "    json.dump(all_results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file containing the Textract output\n",
    "with open('textract_output.json', 'r') as json_file:\n",
    "    textract_response = json.load(json_file)\n",
    "\n",
    "def extract_tables_from_json(response):\n",
    "    # Create a dictionary to map Block Ids to Blocks for faster lookup\n",
    "    block_map = {block['Id']: block for block in response['Blocks']}\n",
    "    \n",
    "    tables = []\n",
    "    current_table = {}\n",
    "    current_page = 1\n",
    "    \n",
    "    for block in response['Blocks']:\n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            table = {}\n",
    "            for relationship in block.get('Relationships', []):\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    cell_ids = relationship['Ids']\n",
    "                    for cell_id in cell_ids:\n",
    "                        cell_block = block_map.get(cell_id)\n",
    "                        if cell_block and cell_block['BlockType'] == 'CELL':\n",
    "                            row = cell_block['RowIndex']\n",
    "                            col = cell_block['ColumnIndex']\n",
    "                            cell_text = ''\n",
    "                            \n",
    "                            # Get the text from the WORD blocks inside the CELL block\n",
    "                            for rel in cell_block.get('Relationships', []):\n",
    "                                if rel['Type'] == 'CHILD':\n",
    "                                    for word_id in rel['Ids']:\n",
    "                                        word_block = block_map.get(word_id)\n",
    "                                        if word_block and word_block['BlockType'] == 'WORD':\n",
    "                                            cell_text += word_block['Text'] + ' '\n",
    "                            \n",
    "                            # Add the cell text to the table dictionary\n",
    "                            if row not in table:\n",
    "                                table[row] = {}\n",
    "                            table[row][col] = cell_text.strip()\n",
    "            \n",
    "            # Check if the table belongs to the current page\n",
    "            if block.get('Page') == current_page:\n",
    "                # Merge current table with the new one if it's on the same page\n",
    "                current_table = merge_tables(current_table, table)\n",
    "            else:\n",
    "                # If page number changes, push the current table to the list and reset\n",
    "                if current_table:\n",
    "                    tables.append(current_table)\n",
    "                current_table = table\n",
    "                current_page = block.get('Page', current_page)\n",
    "\n",
    "    # Append the last table after iteration\n",
    "    if current_table:\n",
    "        tables.append(current_table)\n",
    "        \n",
    "    return tables\n",
    "\n",
    "def merge_tables(existing_table, new_table):\n",
    "    \"\"\"Merges two tables by appending rows from the new table to the existing table.\"\"\"\n",
    "    merged_table = existing_table.copy()\n",
    "    \n",
    "    for row, cols in new_table.items():\n",
    "        if row in merged_table:\n",
    "            # If row exists, merge column values\n",
    "            merged_table[row].update(cols)\n",
    "        else:\n",
    "            # Add the new row if not present\n",
    "            merged_table[row] = cols\n",
    "            \n",
    "    return merged_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file containing the Textract output\n",
    "with open('textract_output.json', 'r') as json_file:\n",
    "    textract_response = json.load(json_file)\n",
    "\n",
    "def extract_tables_from_json(response):\n",
    "    # Create a dictionary to map Block Ids to Blocks for faster lookup\n",
    "    block_map = {block['Id']: block for block in response['Blocks']}\n",
    "    \n",
    "    tables = []\n",
    "    for block in response['Blocks']:\n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            table = {}\n",
    "            for relationship in block.get('Relationships', []):\n",
    "                if relationship['Type'] == 'CHILD':\n",
    "                    cell_ids = relationship['Ids']\n",
    "                    for cell_id in cell_ids:\n",
    "                        cell_block = block_map.get(cell_id)\n",
    "                        if cell_block and cell_block['BlockType'] == 'CELL':\n",
    "                            row = cell_block['RowIndex']\n",
    "                            col = cell_block['ColumnIndex']\n",
    "                            cell_text = ''\n",
    "                            \n",
    "                            # Get the text from the WORD blocks inside the CELL block\n",
    "                            for rel in cell_block.get('Relationships', []):\n",
    "                                if rel['Type'] == 'CHILD':\n",
    "                                    for word_id in rel['Ids']:\n",
    "                                        word_block = block_map.get(word_id)\n",
    "                                        if word_block and word_block['BlockType'] == 'WORD':\n",
    "                                            cell_text += word_block['Text'] + ' '\n",
    "                            \n",
    "                            # Add the cell text to the table dictionary\n",
    "                            if row not in table:\n",
    "                                table[row] = {}\n",
    "                            table[row][col] = cell_text.strip()\n",
    "            tables.append(table)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function to extract tables\n",
    "tables = extract_tables_from_json(textract_response)\n",
    "\n",
    "# Display the extracted table data as DataFrames for better readability\n",
    "for table in tables:\n",
    "    # Convert the extracted table into a DataFrame\n",
    "    df = pd.DataFrame.from_dict(table, orient='index')\n",
    "    #print(df)\n",
    "\n",
    "# Set the first row as the column headers\n",
    "df.columns = df.iloc[0]  # Use the first row as header\n",
    "df = df.drop(df.index[0])  # Drop the first row from the DataFrame\n",
    "\n",
    "# Remove duplicate claims\n",
    "df = df.drop_duplicates(subset='Claim Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>Claim Number</th>\n",
       "      <th>Policy Insured Code</th>\n",
       "      <th>Dealer / Lessee Name</th>\n",
       "      <th>VIN</th>\n",
       "      <th>Subline</th>\n",
       "      <th>State / Province</th>\n",
       "      <th>Cause of Loss</th>\n",
       "      <th>Loss Date</th>\n",
       "      <th>Claim Close Date</th>\n",
       "      <th>Units</th>\n",
       "      <th>Indemnity Payments</th>\n",
       "      <th>Indemnity Expenses</th>\n",
       "      <th>Net Insurance Recoveries</th>\n",
       "      <th>Indemnity Payments Net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150123008611</td>\n",
       "      <td>EKA119</td>\n",
       "      <td>ANDERSON FORD OF ST JOSEP</td>\n",
       "      <td>2LMDU88C77BJ14427</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>Collision</td>\n",
       "      <td>01/23/2015</td>\n",
       "      <td>02/19/2021</td>\n",
       "      <td>0</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161216008812</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>2LMHJ5AT5CBL55030</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Collision</td>\n",
       "      <td>12/16/2016</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$607.50</td>\n",
       "      <td>$2,002.46</td>\n",
       "      <td>$607.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>190806002768</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>MULTI</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Wind</td>\n",
       "      <td>08/06/2019</td>\n",
       "      <td>12/23/2020</td>\n",
       "      <td>8</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$1,677.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>190927007769</td>\n",
       "      <td>EKAA4G</td>\n",
       "      <td>ANDERSON FORD OF GRAND ISLAND</td>\n",
       "      <td>2FMPK4J91KBB61371</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Theft</td>\n",
       "      <td>09/27/2019</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$586.07</td>\n",
       "      <td>$35,200.00</td>\n",
       "      <td>$586.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200118002308</td>\n",
       "      <td>EKA119</td>\n",
       "      <td>ANDERSON FORD OF ST JOSEP</td>\n",
       "      <td>1FT8W3DT3HEC08556</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>Theft</td>\n",
       "      <td>02/18/2017</td>\n",
       "      <td>09/17/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$27,750.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200206005764</td>\n",
       "      <td>EKA894</td>\n",
       "      <td>ANDERSON FORD OF LINCOLN / ANDERSON LINCOLN OF...</td>\n",
       "      <td>JN8AT2MV6HW016228</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>NE</td>\n",
       "      <td>Collision</td>\n",
       "      <td>02/06/2020</td>\n",
       "      <td>02/11/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200218006998</td>\n",
       "      <td>EKA965</td>\n",
       "      <td>ANDERSON KIA</td>\n",
       "      <td>5XYPHDA52LG647197</td>\n",
       "      <td>FCNAUS</td>\n",
       "      <td>MO</td>\n",
       "      <td>All Other (non WX)</td>\n",
       "      <td>02/18/2020</td>\n",
       "      <td>04/07/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "      <td>$0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "1   Claim Number Policy Insured Code  \\\n",
       "2   150123008611              EKA119   \n",
       "4   161216008812              EKAA4G   \n",
       "6   190806002768              EKAA4G   \n",
       "8   190927007769              EKAA4G   \n",
       "10  200118002308              EKA119   \n",
       "12  200206005764              EKA894   \n",
       "14  200218006998              EKA965   \n",
       "\n",
       "1                                Dealer / Lessee Name                VIN  \\\n",
       "2                           ANDERSON FORD OF ST JOSEP  2LMDU88C77BJ14427   \n",
       "4                       ANDERSON FORD OF GRAND ISLAND  2LMHJ5AT5CBL55030   \n",
       "6                       ANDERSON FORD OF GRAND ISLAND              MULTI   \n",
       "8                       ANDERSON FORD OF GRAND ISLAND  2FMPK4J91KBB61371   \n",
       "10                          ANDERSON FORD OF ST JOSEP  1FT8W3DT3HEC08556   \n",
       "12  ANDERSON FORD OF LINCOLN / ANDERSON LINCOLN OF...  JN8AT2MV6HW016228   \n",
       "14                                       ANDERSON KIA  5XYPHDA52LG647197   \n",
       "\n",
       "1  Subline State / Province       Cause of Loss   Loss Date Claim Close Date  \\\n",
       "2   FCNAUS               MO           Collision  01/23/2015       02/19/2021   \n",
       "4   FCNAUS               NE           Collision  12/16/2016                    \n",
       "6   FCNAUS               NE                Wind  08/06/2019       12/23/2020   \n",
       "8   FCNAUS               NE               Theft  09/27/2019                    \n",
       "10  FCNAUS               MO               Theft  02/18/2017       09/17/2020   \n",
       "12  FCNAUS               NE           Collision  02/06/2020       02/11/2020   \n",
       "14  FCNAUS               MO  All Other (non WX)  02/18/2020       04/07/2020   \n",
       "\n",
       "1  Units Indemnity Payments Indemnity Expenses Net Insurance Recoveries  \\\n",
       "2      0              $0.00              $0.00                    $0.00   \n",
       "4      0              $0.00            $607.50                $2,002.46   \n",
       "6      8              $0.00              $0.00                $1,677.00   \n",
       "8      1              $0.00            $586.07               $35,200.00   \n",
       "10     1              $0.00              $0.00               $27,750.00   \n",
       "12     1              $0.00              $0.00                    $0.00   \n",
       "14     1              $0.00              $0.00                    $0.00   \n",
       "\n",
       "1  Indemnity Payments Net  \n",
       "2                   $0.00  \n",
       "4                 $607.50  \n",
       "6                   $0.00  \n",
       "8                 $586.07  \n",
       "10                  $0.00  \n",
       "12                  $0.00  \n",
       "14                  $0.00  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161216008812 EKAA4G ANDERSON FORD OF GRAND ISLAND 2LMHJ5AT5CBL55030 FCNAUS NE Collision 12/16/2016  0 $0.00 $607.50 $2,002.46 $607.50'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'161216008812 EKAA4G ANDERSON FORD OF GRAND ISLAND 2LMHJ5AT5CBL55030 FCNAUS NE Collision 12/16/2016  0    '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern to match prices\n",
    "pattern = r'\\$([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(\\.[0-9]{2})?'\n",
    "\n",
    "# Removing prices from the text column\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "df.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for batch 1:\n",
      "['Relevant']\n",
      "Response for batch 2:\n",
      "['Relevant']\n",
      "Response for batch 3:\n",
      "['Relevant']\n",
      "Response for batch 4:\n",
      "['Relevant']\n",
      "Response for batch 5:\n",
      "['Relevant']\n",
      "Response for batch 6:\n",
      "['Relevant']\n",
      "Response for batch 7:\n",
      "['Relevant']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)  # 2 labels: relevant or not\n",
    "\n",
    "# Define the prompt (your task description and data)\n",
    "prompt_template = \"\"\"\n",
    "Given the following claims data, remove all claims not relevant to open dealer lot coverage or that involve third-party coverage.\n",
    "Return the Claim Number of the remaining claims along with a short description of why each was included.\n",
    "CLAIM DATA GOES HERE\n",
    "\"\"\"\n",
    "\n",
    "# Function to count tokens using Hugging Face tokenizer\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return tokens.size(1)  # Return the token count\n",
    "\n",
    "# Function to batch the DataFrame\n",
    "def batch_dataframe(df, max_tokens, prompt_tokens):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_str = row.to_string(index=False, header=False)  # Convert row to string\n",
    "        row_tokens = count_tokens(row_str)\n",
    "\n",
    "        # Check if adding this row exceeds the limit\n",
    "        if current_tokens + row_tokens + prompt_tokens > max_tokens:\n",
    "            if current_batch:  # If there's already data in the current batch\n",
    "                batches.append(pd.DataFrame(current_batch))\n",
    "                current_batch = []  # Reset current batch\n",
    "                current_tokens = 0\n",
    "\n",
    "        current_batch.append(row)\n",
    "        current_tokens += row_tokens\n",
    "\n",
    "    if current_batch:  # Add any remaining data\n",
    "        batches.append(pd.DataFrame(current_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "# Generate responses for each batch\n",
    "def process_batches(batches, prompt_template, model, tokenizer, max_length=200):\n",
    "    responses = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        data_string = batch.to_string(index=False, header=True)  # Create the data string from the batch\n",
    "        \n",
    "        # Create the full prompt with data included\n",
    "        full_prompt = prompt_template.replace(\"CLAIM DATA GOES HERE\", data_string)  # Replace placeholder with data\n",
    "        \n",
    "        try:\n",
    "            response = generate_response(full_prompt, model, tokenizer, max_length)\n",
    "            responses.append((i + 1, response))  # Append the batch number and response\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i + 1}: {e}\")\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Function to generate a response from the model\n",
    "def generate_response(prompt, model, tokenizer, max_length=200):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "\n",
    "    # Decode the generated tokens to get the output text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Calculate prompt tokens and convert the entire DataFrame into batches\n",
    "prompt_tokens = count_tokens(prompt_template)\n",
    "max_tokens = 512  # Adjust based on model limit\n",
    "df_batches = batch_dataframe(df, max_tokens, prompt_tokens)\n",
    "\n",
    "# Process the batches and get responses\n",
    "responses = process_batches(df_batches, prompt_template, model, tokenizer)\n",
    "\n",
    "# Print or handle the responses\n",
    "for batch_num, response in responses:\n",
    "    print(f\"Response for batch {batch_num}:\")\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at llmware/industry-bert-insurance-v0.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"llmware/industry-bert-insurance-v0.1\")\n",
    "\n",
    "def create_prompt(claim):\n",
    "    return f\"Evaluate the following claim for relevance to open dealer lot coverage excluding third parties:{claim}\\nIs this claim relevant? (yes/no)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim 150123008611 Logits: tensor([[ 0.1962, -0.2636]]), Probabilities: tensor([[0.6130, 0.3870]])\n",
      "Claim 161216008812 Logits: tensor([[ 0.1819, -0.2930]]), Probabilities: tensor([[0.6165, 0.3835]])\n",
      "Claim 190806002768 Logits: tensor([[ 0.1818, -0.2586]]), Probabilities: tensor([[0.6084, 0.3916]])\n",
      "Claim 190927007769 Logits: tensor([[ 0.1822, -0.2629]]), Probabilities: tensor([[0.6095, 0.3905]])\n",
      "Claim 200118002308 Logits: tensor([[ 0.2032, -0.2967]]), Probabilities: tensor([[0.6224, 0.3776]])\n",
      "Claim 200206005764 Logits: tensor([[ 0.1700, -0.3021]]), Probabilities: tensor([[0.6159, 0.3841]])\n",
      "Claim 200218006998 Logits: tensor([[ 0.1874, -0.2653]]), Probabilities: tensor([[0.6113, 0.3887]])\n",
      "Filtered Claims:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Classify claims\n",
    "def classify_claims(df):\n",
    "    relevant_claims = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = create_prompt(row)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = F.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
    "            \n",
    "            # Print logits and probabilities for debugging\n",
    "            print(f\"Claim {row['Claim Number']} Logits: {logits}, Probabilities: {probabilities}\")\n",
    "\n",
    "            predicted_class = logits.argmax().item()  # Get the predicted class index\n",
    "\n",
    "        # Assuming class index 1 is \"relevant\"\n",
    "        if predicted_class == 1:  # Adjust based on your model's output class index\n",
    "            relevant_claims.append(row)\n",
    "\n",
    "    return pd.DataFrame(relevant_claims)\n",
    "\n",
    "# Get relevant claims\n",
    "relevant_claims_df = classify_claims(df)\n",
    "\n",
    "# Print the filtered claims\n",
    "print(\"Filtered Claims:\")\n",
    "print(relevant_claims_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from google.oauth2 import service_account\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# Prepare variables\n",
    "project_id = 'verdant-cargo-443521-j3'\n",
    "location = 'us'\n",
    "processor_id = 'f2db9bcc34ed8bb5'\n",
    "\n",
    "file_path = '/path/to/local/file/.pdf'\n",
    "mime_type = 'application/pdf'\n",
    "\n",
    "# Load the service account key\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"../secret/verdant-cargo-443521-j3-8d56893a2e2e.json\"\n",
    ")\n",
    "\n",
    "opts = {\n",
    "    \"api_endpoint\" : f\"{location}-documentai.googleapis.com\"\n",
    "}\n",
    "\n",
    "# Configure the processor client (i.e. prepare the endpoint)\n",
    "client = documentai.DocumentProcessorServiceClient(client_options=opts, credentials=credentials)\n",
    "\n",
    "name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "#Open File\n",
    "with open('../loss_runs/input/Loss_Run___len stoler 8-24_page_5.pdf', 'rb') as pdf_file:\n",
    "    pdf_data = pdf_file.read()\n",
    "\n",
    "# Construct the request\n",
    "raw_document = documentai.RawDocument(content=pdf_data, mime_type=mime_type)\n",
    "\n",
    "request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "\n",
    "# Analyze output\n",
    "result = client.process_document(request=request)\n",
    "\n",
    "document = result.document\n",
    "print(document)\n",
    "\n",
    "with open('../documentai_results/result.json', 'w') as file:\n",
    "    json.dump(document, file, indent=4)  # Writes the Python dict to the file in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/15/24 8:56 AM\n",
      "Loss Run: By Policy Year\n",
      "Page 5 of 16\n",
      "⑳\n",
      "ZURICHⓇ\n",
      "Site Code 03\n",
      "Lexus\n",
      "Val ID\n",
      "Date of\n",
      "Claimant\n",
      "Paid\n",
      "Paid\n",
      "Reserves\n",
      "Claim\n",
      "Net\n",
      "Claim\n",
      "Number\n",
      "Coverage\n",
      "Sub Coverage\n",
      "Description\n",
      "Loss\n",
      "Status\n",
      "Name\n",
      "Accident Narrative\n",
      "Indemnity\n",
      "Expense\n",
      "Total\n",
      "Recovery\n",
      "Incurred\n",
      "AUTO\n",
      "PHYSICAL\n",
      "DAMAGE\n",
      "Accident with dump\n",
      "truck.\n",
      "AUTOMOBILE\n",
      "DEMO COLLISION\n",
      "1510180511\n",
      "20220210\n",
      "Closed\n",
      "Len Stoler Inc\n",
      "$19,381.40\n",
      "$160.80\n",
      "$0.00\n",
      "$0.00\n",
      "$19,542.20\n",
      "AUTO\n",
      "PHYSICAL\n",
      "DAMAGE\n",
      "Len Stoler\n",
      "SERVICE VEHICLE\n",
      "COLLISION\n",
      "OV at stoplight\n",
      "completely stopped and\n",
      "was rear ended by IV.\n",
      "AUTOMOBILE\n",
      "1510180943\n",
      "20220225\n",
      "Closed\n",
      "Automotiv e\n",
      "$0.00\n",
      "$230.50\n",
      "$0.00\n",
      "$0.00\n",
      "$230.50\n",
      "GARAGE LIABILITY -\n",
      "GENERAL\n",
      "LIABILITY\n",
      "GARAGE\n",
      "LIABILITY\n",
      "Accident with dump\n",
      "truck.\n",
      "PD\n",
      "4620220783\n",
      "Baltimore\n",
      "County Gov\n",
      "20220210\n",
      "Closed\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "Claim Count:\n",
      "3\n",
      "Sum:\n",
      "$19,381.40\n",
      "$391.30\n",
      "$0.00\n",
      "$0.00\n",
      "$19,772.70\n",
      "Site Code 05\n",
      "Porsche Audi\n",
      "Val ID\n",
      "Claim\n",
      "Date of\n",
      "Claimant\n",
      "Paid\n",
      "Paid\n",
      "Reserves\n",
      "Claim\n",
      "Net\n",
      "Coverage\n",
      "Sub Coverage\n",
      "Description\n",
      "Number\n",
      "Loss\n",
      "Status\n",
      "Name\n",
      "Accident Narrative\n",
      "Indemnity\n",
      "Expense\n",
      "Total\n",
      "Recovery\n",
      "Incurred\n",
      "AUTO\n",
      "PHYSICAL\n",
      "DAMAGE\n",
      "IV rear ended by OV at\n",
      "intersection.\n",
      "AUTOMOBILE\n",
      "COLLISION\n",
      "1510184079\n",
      "20220225\n",
      "Closed\n",
      "Len Stoler Inc\n",
      "$87,182.00\n",
      "$19.00\n",
      "$0.00 ##########\n",
      "$5,923.00\n",
      "AUTO\n",
      "PHYSICAL\n",
      "USED VEHICLE\n",
      "Len Stoler\n",
      "AUTOMOBILE\n",
      "DAMAGE\n",
      "COMP\n",
      "1510189436\n",
      "20220918\n",
      "Closed\n",
      "Automotiv e\n",
      "Stolen off lot\n",
      "$31,330.65\n",
      "$18.95\n",
      "$0.00\n",
      "$45.00\n",
      "$31,394.60\n",
      "GARAGE\n",
      "GARAGE LIABILITY -\n",
      "GENERAL\n",
      "LIABILITY\n",
      "Lacitignola\n",
      "Malia Z\n",
      "IV rear ended by OV at\n",
      "intersection.\n",
      "LIABILITY\n",
      "PD\n",
      "4620223144\n",
      "20220225\n",
      "Closed\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "$0.00\n",
      "Claim Count:\n",
      "3\n",
      "Sum:\n",
      "$118,512.65\n",
      "$37.95\n",
      "$0.00 ##########\n",
      "$37,317.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(document.text)\n",
    "for page in document.pages:\n",
    "    for table in page.tables:\n",
    "        #print(table.header_rows)\n",
    "        for row in table.header_rows:\n",
    "            for cell in row.cells:\n",
    "                cell_text = \"\"\n",
    "                for text_segment in cell.layout.text_anchor.text_segments:\n",
    "                    cell_text += document.text[text_segment.start_index:text_segment.end_index]\n",
    "                #print(cell_text.strip())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5fc3a238adc25dec0d5b2079827dca283ce2d8f5922d8045d64c5203803846b"
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
